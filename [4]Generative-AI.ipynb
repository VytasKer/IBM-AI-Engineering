{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI and LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install spacy\n",
    "!pip install numpy==1.24\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n",
    "!pip install numpy scikit-learn\n",
    "!pip install torch==2.0.1\n",
    "!pip install torchtext==0.15.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from transformers import BertTokenizer\n",
    "from transformers import XLNetTokenizer\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Building a simple chatbot with transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq tensorflow\n",
    "!pip install -qq transformers\n",
    "!pip install sentencepiece\n",
    "!pip install torch==2.0.1\n",
    "!pip install torchtext==0.15.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Selecting the model. You will be using \"facebook/blenderbot-400M-distill\" in this example.\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chat function\n",
    "def chat_with_bot():\n",
    "    while True:\n",
    "        # Get user input\n",
    "        input_text = input(\"You: \")\n",
    "\n",
    "        # Exit conditions\n",
    "        if input_text.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Tokenize input and generate response\n",
    "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs, max_new_tokens=150) \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # Display bot's response\n",
    "        print(\"Chatbot:\", response)\n",
    "\n",
    "# Start chatting\n",
    "chat_with_bot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a sample sentence for word tokenization.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "# This showcases word_tokenize from nltk library\n",
    "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "# This showcases the use of the 'spaCy' tokenizer with torchtext's get_tokenizer function\n",
    "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Making a list of the tokens and priting the list\n",
    "token_list = [token.text for token in doc]\n",
    "print(\"Tokens:\", token_list)\n",
    "\n",
    "# Showing token details\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"),\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"),\n",
    "    (3,\"Sentiment Analysis using PyTorch\"),\n",
    "    (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
    "    (1,\" Machine Translation with NLP \"),\n",
    "    (1,\" Named Entity vs Sentiment Analysis  NLP \")]\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokenizer(dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for  _,text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "my_iterator = yield_tokens(dataset)\n",
    "next(my_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code demonstrates how to fetch a tokenized sentence from an iterator, convert its \n",
    "# tokens into indices using a provided vocabulary, and then print both the original \n",
    "# sentence and its corresponding indices.\n",
    "def get_tokenized_sentence_and_indices(iterator):\n",
    "    tokenized_sentence = next(iterator)  # Get the next tokenized sentence\n",
    "    token_indices = [vocab[token] for token in tokenized_sentence]  # Get token indices\n",
    "    return tokenized_sentence, token_indices\n",
    "\n",
    "tokenized_sentence, token_indices = get_tokenized_sentence_and_indices(my_iterator)\n",
    "next(my_iterator)\n",
    "\n",
    "print(\"Tokenized Sentence:\", tokenized_sentence)\n",
    "print(\"Token Indices:\", token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\"IBM taught me tokenization\", \n",
    "         \"Special tokenizers are ready and they will blow your mind\", \n",
    "         \"just saying hi!\"]\n",
    "\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "tokens = []\n",
    "max_length = 0\n",
    "\n",
    "for line in lines:\n",
    "    tokenized_line = tokenizer_en(line)\n",
    "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
    "    tokens.append(tokenized_line)\n",
    "    max_length = max(max_length, len(tokenized_line))\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))\n",
    "\n",
    "print(\"Lines after adding special tokens:\\n\", tokens)\n",
    "\n",
    "# Build vocabulary without unk_init\n",
    "vocab = build_vocab_from_iterator(tokens, specials=['<unk>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Vocabulary and Token Ids\n",
    "print(\"Vocabulary:\", vocab.get_itos())\n",
    "print(\"Token IDs for 'tokenization':\", vocab.get_stoi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_line = \"I learned about embeddings and attention mechanisms.\"\n",
    "\n",
    "# Tokenize the new line\n",
    "tokenized_new_line = tokenizer_en(new_line)\n",
    "tokenized_new_line = ['<bos>'] + tokenized_new_line + ['<eos>']\n",
    "\n",
    "# Pad the new line to match the maximum length of previous lines\n",
    "new_line_padded = tokenized_new_line + ['<pad>'] * (max_length - len(tokenized_new_line))\n",
    "\n",
    "# Convert tokens to IDs and handle unknown words\n",
    "new_line_ids = [vocab[token] if token in vocab else vocab['<unk>'] for token in new_line_padded]\n",
    "\n",
    "# Example usage\n",
    "print(\"Token IDs for new line:\", new_line_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative text tokenization and performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "import nltk\n",
    "import spacy\n",
    "from transformers import BertTokenizer, XLNetTokenizer\n",
    "from datetime import datetime\n",
    "\n",
    "# NLTK Tokenization\n",
    "start_time = datetime.now()\n",
    "nltk_tokens = nltk.word_tokenize(text)\n",
    "nltk_time = datetime.now() - start_time\n",
    "\n",
    "# SpaCy Tokenization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "start_time = datetime.now()\n",
    "spacy_tokens = [token.text for token in nlp(text)]\n",
    "spacy_time = datetime.now() - start_time\n",
    "\n",
    "# BertTokenizer Tokenization\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "start_time = datetime.now()\n",
    "bert_tokens = bert_tokenizer.tokenize(text)\n",
    "bert_time = datetime.now() - start_time\n",
    "\n",
    "# XLNetTokenizer Tokenization\n",
    "xlnettokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "start_time = datetime.now()\n",
    "xlnet_tokens = xlnettokenizer.tokenize(text)\n",
    "xlnet_time = datetime.now() - start_time\n",
    "\n",
    "# Display tokens, time taken for each tokenizer, and token frequencies\n",
    "print(f\"NLTK Tokens: {nltk_tokens}\\nTime Taken: {nltk_time} seconds\\n\")\n",
    "show_frequencies(nltk_tokens, \"NLTK\")\n",
    "\n",
    "print(f\"SpaCy Tokens: {spacy_tokens}\\nTime Taken: {spacy_time} seconds\\n\")\n",
    "show_frequencies(spacy_tokens, \"SpaCy\")\n",
    "\n",
    "print(f\"Bert Tokens: {bert_tokens}\\nTime Taken: {bert_time} seconds\\n\")\n",
    "show_frequencies(bert_tokens, \"Bert\")\n",
    "\n",
    "print(f\"XLNet Tokens: {xlnet_tokens}\\nTime Taken: {xlnet_time} seconds\\n\")\n",
    "show_frequencies(xlnet_tokens, \"XLNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an NLP Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
    "    \"Fame's a fickle friend, Harry.\",\n",
    "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
    "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
    "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
    "    \"You are awesome!\"\n",
    "]\n",
    "\n",
    "# Define a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "\n",
    "# Create an instance of your custom dataset\n",
    "custom_dataset = CustomDataset(sentences)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 2\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
    "    \"Fame's a fickle friend, Harry.\",\n",
    "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
    "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
    "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
    "    \"You are awesome!\"\n",
    "]\n",
    "\n",
    "# Define a custom data set\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, vocab):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer(self.sentences[idx])\n",
    "        # Convert tokens to tensor indices using vocab\n",
    "        tensor_indices = [self.vocab[token] for token in tokens]\n",
    "        return torch.tensor(tensor_indices)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, sentences))\n",
    "\n",
    "# Create an instance of your custom data set\n",
    "custom_dataset = CustomDataset(sentences, tokenizer, vocab)\n",
    "\n",
    "print(\"Custom Dataset Length:\", len(custom_dataset))\n",
    "print(\"Sample Items:\")\n",
    "for i in range(6):\n",
    "    sample_item = custom_dataset[i]\n",
    "    print(f\"Item {i + 1}: {sample_item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your custom data set\n",
    "custom_dataset = CustomDataset(sentences, tokenizer, vocab)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 2\n",
    "\n",
    "# Create a data loader\n",
    "#dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate through the data loader\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom collate function\n",
    "def collate_fn(batch):\n",
    "    # Pad sequences within the batch to have equal lengths\n",
    "    padded_batch = pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "    return padded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader with the custom collate function with batch_first=True,\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# Iterate through the data loader\n",
    "for batch in dataloader: \n",
    "    for row in batch:\n",
    "        for idx in row:\n",
    "            words = [vocab.get_itos()[idx] for idx in row]\n",
    "        print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def plot(COST,ACC):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(COST, color=color)\n",
    "    ax1.set_xlabel('epoch', color=color)\n",
    "    ax1.set_ylabel('total loss', color=color)\n",
    "    ax1.tick_params(axis='y', color=color)\n",
    "    \n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('accuracy', color=color)  # you already handled the x-label with ax1\n",
    "    ax2.plot(ACC, color=color)\n",
    "    ax2.tick_params(axis='y', color=color)\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter= iter(AG_NEWS(split=\"train\"))\n",
    "\n",
    "y,text= next((train_iter))\n",
    "print(y,text)\n",
    "\n",
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "ag_news_label[y]\n",
    "\n",
    "num_class = len(set([label for (label, text) in train_iter ]))\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for  _,text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "vocab([\"age\",\"hello\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing iterators.\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "\n",
    "# Convert the training and testing iterators to map-style datasets.\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Determine the number of samples to be used for training and validation (5% for validation).\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "\n",
    "# Randomly split the training dataset into training and validation datasets using `random_split`.\n",
    "# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emsize=64\n",
    "\n",
    "vocab_size=len(vocab)\n",
    "\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count= 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=0.1\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "cum_loss_list=[]\n",
    "acc_epoch=[]\n",
    "acc_old=0\n",
    "\n",
    "for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "    model.train()\n",
    "    cum_loss=0\n",
    "    for idx, (label, text, offsets) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        cum_loss+=loss.item()\n",
    "\n",
    "    cum_loss_list.append(cum_loss)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    acc_epoch.append(accu_val)\n",
    "\n",
    "    if accu_val > acc_old:\n",
    "      acc_old= accu_val\n",
    "      torch.save(model.state_dict(), 'my_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(article, text_pipeline)\n",
    "\n",
    "markdown_content = f'''\n",
    "<div style=\"background-color: lightgray; padding: 10px;\">\n",
    "    <h3>{article}</h3>\n",
    "    <h4>The category of the news article: {result}</h4>\n",
    "</div>\n",
    "'''\n",
    "\n",
    "md(markdown_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Training a Simple Language Model with a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def preprocess_string(s):\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespaces with no space\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "def preprocess(words):\n",
    "    tokens=word_tokenize(words)\n",
    "    tokens=[preprocess_string(w)   for w in tokens]\n",
    "    return [w.lower()  for w in tokens if len(w)!=0 or not(w in string.punctuation) ]\n",
    "\n",
    "tokens=preprocess(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a frequency distribution of words\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "fdist\n",
    "\n",
    "# Plot the top 10 words\n",
    "plt.bar(list(fdist.keys())[0:10],list(fdist.values())[0:10])\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.bigrams(tokens)\n",
    "bigrams\n",
    "\n",
    "my_bigrams=list(nltk.bigrams(tokens))\n",
    "\n",
    "my_bigrams[0:10]\n",
    "\n",
    "freq_bigrams  = nltk.FreqDist(nltk.bigrams(tokens))\n",
    "freq_bigrams\n",
    "\n",
    "freq_bigrams[('we', 'are')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word=\"strangers\"\n",
    "vocab_probabilities={}\n",
    "for next_word in vocabulary:\n",
    "    vocab_probabilities[next_word]=freq_bigrams[(word,next_word)]/fdist[word]\n",
    "\n",
    "vocab_probabilities=sorted(vocab_probabilities.items(), key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(my_words, freq_grams, normlize=1, vocabulary=vocabulary):\n",
    "    \"\"\"\n",
    "    Generate predictions for the conditional probability of the next word given a sequence.\n",
    "\n",
    "    Args:\n",
    "        my_words (list): A list of words in the input sequence.\n",
    "        freq_grams (dict): A dictionary containing frequency of n-grams.\n",
    "        normlize (int): A normalization factor for calculating probabilities.\n",
    "        vocabulary (list): A list of words in the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of predicted words along with their probabilities, sorted in descending order.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_probabilities = {}  # Initialize a dictionary to store predicted word probabilities\n",
    "\n",
    "    context_size = len(list(freq_grams.keys())[0])  # Determine the context size from n-grams keys\n",
    "\n",
    "    # Preprocess input words and take only the relevant context words\n",
    "    my_tokens = preprocess(my_words)[0:context_size - 1]\n",
    "\n",
    "    # Calculate probabilities for each word in the vocabulary given the context\n",
    "    for next_word in vocabulary:\n",
    "        temp = my_tokens.copy()\n",
    "        temp.append(next_word)  # Add the next word to the context\n",
    "\n",
    "        # Calculate the conditional probability using the frequency information\n",
    "        if normlize!=0:\n",
    "            vocab_probabilities[next_word] = freq_grams[tuple(temp)] / normlize\n",
    "        else:\n",
    "            vocab_probabilities[next_word] = freq_grams[tuple(temp)] \n",
    "    # Sort the predicted words based on their probabilities in descending order\n",
    "    vocab_probabilities = sorted(vocab_probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return vocab_probabilities  # Return the sorted list of predicted words and their probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.context_size=context_size\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds=torch.reshape( embeds, (-1,self.context_size * self.embedding_dim))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_song(model,number_of_words=100):\n",
    "    my_song=\"\"\n",
    "    for i in range(number_of_words):\n",
    "        with torch.no_grad():\n",
    "            context=torch.tensor(vocab([tokens[i-j-1] for j in range(CONTEXT_SIZE)])).to(device)\n",
    "            word_inx=torch.argmax(model(context))\n",
    "            my_song+=\" \"+index_to_token[word_inx.detach().item()]\n",
    "\n",
    "    return my_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, number_of_epochs=100, show=10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataloader (DataLoader): DataLoader containing training data.\n",
    "        model (nn.Module): Neural network model to be trained.\n",
    "        number_of_epochs (int, optional): Number of epochs for training. Default is 100.\n",
    "        show (int, optional): Interval for displaying progress. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        list: List containing loss values for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    MY_LOSS = []  # List to store loss values for each epoch\n",
    "\n",
    "    # Iterate over the specified number of epochs\n",
    "    for epoch in tqdm(range(number_of_epochs)):\n",
    "        total_loss = 0  # Initialize total loss for the current epoch\n",
    "        my_song = \"\"    # Initialize a string to store the generated song\n",
    "\n",
    "        # Iterate over batches in the dataloader\n",
    "        for context, target in dataloader:\n",
    "            model.zero_grad()          # Zero the gradients to avoid accumulation\n",
    "            predicted = model(context)  # Forward pass through the model to get predictions\n",
    "            loss = criterion(predicted, target.reshape(-1))  # Calculate the loss\n",
    "            total_loss += loss.item()   # Accumulate the loss\n",
    "\n",
    "            loss.backward()    # Backpropagation to compute gradients\n",
    "            optimizer.step()   # Update model parameters using the optimizer\n",
    "\n",
    "        # Display progress and generate song at specified intervals\n",
    "        if epoch % show == 0:\n",
    "            my_song += write_song(model)  # Generate song using the model\n",
    "\n",
    "            print(\"Generated Song:\")\n",
    "            print(\"\\n\")\n",
    "            print(my_song)\n",
    "\n",
    "        MY_LOSS.append(total_loss/len(dataloader))  # Append the total loss for the epoch to MY_LOSS list\n",
    "\n",
    "    return MY_LOSS  # Return the list of  mean loss values for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the context size for the n-gram model\n",
    "CONTEXT_SIZE = 2\n",
    "\n",
    "# Create an instance of the NGramLanguageModeler class with specified parameters\n",
    "model_2 = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)\n",
    "\n",
    "# Define the optimizer for training the model, using stochastic gradient descent (SGD)\n",
    "optimizer = optim.SGD(model_2.parameters(), lr=0.01)\n",
    "\n",
    "# Set up a learning rate scheduler using StepLR to adjust the learning rate during training\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_path = '2gram.pth'\n",
    "torch.save(model_2.state_dict(), save_path)\n",
    "my_loss_list.append(my_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_2.embeddings.weight.cpu().detach().numpy()\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "\n",
    "labels = []\n",
    "\n",
    "for j in range(len(X_2d)):\n",
    "    if j < 20:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1], label=index_to_token[j])\n",
    "        labels.append(index_to_token[j])\n",
    "        # Add words as annotations\n",
    "        plt.annotate(index_to_token[j],\n",
    "                     (X_2d[j, 0], X_2d[j, 1]),\n",
    "                     textcoords=\"offset points\",\n",
    "                     xytext=(0, 10),\n",
    "                     ha='center')\n",
    "    else:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1])\n",
    "\n",
    "plt.legend(labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (my_loss, model_name)in zip(my_loss_list,[\"2-gram\",\"4-gram\",\"8-gram\"]):\n",
    "    plt.plot(my_loss,label=\"Cross-entropy Loss - {}\".format(model_name))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity\n",
    "for (my_loss, model_name)in zip(my_loss_list,[\"2-gram\",\"4-gram\",\"8-gram\"]):\n",
    "    # Calculate perplexity using the loss\n",
    "    perplexity = np.exp(my_loss)\n",
    "    plt.plot(perplexity,label=\"Perplexity - {}\".format(model_name))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(word_embeddings,vocab=vocab):\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    word_embeddings_2d = tsne.fit_transform(word_embeddings)\n",
    "\n",
    "    # Plotting the results with labels from vocab\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i, word in enumerate(vocab.get_itos() ):  # assuming vocab.itos gives the list of words in your vocab\n",
    "        plt.scatter(word_embeddings_2d[i, 0], word_embeddings_2d[i, 1])\n",
    "        plt.annotate(word, (word_embeddings_2d[i, 0], word_embeddings_2d[i, 1]))\n",
    "\n",
    "    plt.xlabel(\"t-SNE component 1\")\n",
    "    plt.ylabel(\"t-SNE component 2\")\n",
    "    plt.title(\"Word Embeddings visualized with t-SNE\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the most similar words to a target word by calculating word vectors' cosine distance\n",
    "def find_similar_words(word, word_embeddings, top_k=5):\n",
    "    if word not in word_embeddings:\n",
    "        print(\"Word not found in embeddings.\")\n",
    "        return []\n",
    "\n",
    "    # Get the embedding for the given word\n",
    "    target_embedding = word_embeddings[word]\n",
    "\n",
    "    # Calculate cosine similarities between the target word and all other words\n",
    "    similarities = {}\n",
    "    for w, embedding in word_embeddings.items():\n",
    "        if w != word:\n",
    "            similarity = torch.dot(target_embedding, embedding) / (\n",
    "                torch.norm(target_embedding) * torch.norm(embedding)\n",
    "            )\n",
    "            similarities[w] = similarity.item()\n",
    "\n",
    "    # Sort the similarities in descending order\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the top k similar words\n",
    "    most_similar_words = [w for w, _ in sorted_similarities[:top_k]]\n",
    "    return most_similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=1000):\n",
    "    \"\"\"\n",
    "    Train the model for the specified number of epochs.\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model to be trained.\n",
    "        dataloader: DataLoader providing data for training.\n",
    "        criterion: Loss function.\n",
    "        optimizer: Optimizer for updating model's weights.\n",
    "        num_epochs: Number of epochs to train the model for.\n",
    "\n",
    "    Returns:\n",
    "        model: The trained model.\n",
    "        epoch_losses: List of average losses for each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    # List to store running loss for each epoch\n",
    "    epoch_losses = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Storing running loss values for the current epoch\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Using tqdm for a progress bar\n",
    "        for idx, samples in enumerate(dataloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Check for EmbeddingBag layer in the model\n",
    "            if any(isinstance(module, nn.EmbeddingBag) for _, module in model.named_modules()):\n",
    "                target, context, offsets = samples\n",
    "                predicted = model(context, offsets)\n",
    "            \n",
    "            # Check for Embedding layer in the model\n",
    "            elif any(isinstance(module, nn.Embedding) for _, module in model.named_modules()):\n",
    "                target, context = samples\n",
    "                predicted = model(context)\n",
    "                \n",
    "            loss = criterion(predicted, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Append average loss for the epoch\n",
    "        epoch_losses.append(running_loss / len(dataloader))\n",
    "    \n",
    "    return model, epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = \"\"\"I wish I was little bit taller\n",
    "I wish I was a baller\n",
    "She wore a small black dress to the party\n",
    "The dog chased a big red ball in the park\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')  # This uses basic English tokenizer. You can choose another.\n",
    "\n",
    "# Step 2: Tokenize sentences\n",
    "def tokenize_data(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "tokenized_toy_data = tokenizer (toy_data)\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(tokenize_data(tokenized_toy_data), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "sample_sentence = \"I wish I was a baller\"\n",
    "tokenized_sample = tokenizer(sample_sentence)\n",
    "encoded_sample = [vocab[token] for token in tokenized_sample]\n",
    "print(\"Encoded sample:\", encoded_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda tokens:[ vocab[token]  for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE=2\n",
    "\n",
    "cobow_data = []\n",
    "for i in range(1, len(tokenized_toy_data ) - CONTEXT_SIZE):\n",
    "    context = (\n",
    "        [tokenized_toy_data [i - j - 1] for j in range(CONTEXT_SIZE)]\n",
    "        + [tokenized_toy_data [i + j + 1] for j in range(CONTEXT_SIZE)]\n",
    "    )\n",
    "    target = tokenized_toy_data [i]\n",
    "    cobow_data.append((context, target))\n",
    "\n",
    "print(cobow_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    target_list, context_list, offsets = [], [], [0]\n",
    "    for _context, _target in batch:\n",
    "        \n",
    "        target_list.append(vocab[_target])  \n",
    "        processed_context = torch.tensor(text_pipeline(_context), dtype=torch.int64)\n",
    "        context_list.append(processed_context)\n",
    "        offsets.append(processed_context.size(0))\n",
    "    target_list = torch.tensor(target_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    context_list = torch.cat(context_list)\n",
    "    return target_list.to(device), context_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "target_list, context_list, offsets=collate_batch(cobow_data[0:10])\n",
    "print(f\"target_list(Tokenized target words): {target_list} , context_list(Surrounding context words): {context_list} , offsets(Starting indexes of context words for each target): {offsets} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  # batch size for training\n",
    "\n",
    "dataloader_cbow = DataLoader(\n",
    "    cobow_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "print(dataloader_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    # Initialize the CBOW model\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        \n",
    "        super(CBOW, self).__init__()\n",
    "         # Define the embedding layer using nn.EmbeddingBag\n",
    "        # It outputs the average of context words embeddings\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        # Define the first linear layer with input size embed_dim and output size embed_dim//2\n",
    "        self.linear1 = nn.Linear(embed_dim, embed_dim//2)\n",
    "        # Define the fully connected layer with input size embed_dim//2 and output size vocab_size\n",
    "        self.fc = nn.Linear(embed_dim//2, vocab_size)\n",
    "        \n",
    "\n",
    "        self.init_weights()\n",
    "    # Initialize the weights of the model's parameters\n",
    "    def init_weights(self):\n",
    "        # Initialize the weights of the embedding layer\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        # Initialize the weights of the fully connected layer\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        # Initialize the biases of the fully connected layer to zeros\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        # Pass the input text and offsets through the embedding layer\n",
    "        out = self.embedding(text, offsets)\n",
    "        # Apply the ReLU activation function to the output of the first linear layer\n",
    "        out = torch.relu(self.linear1(out))\n",
    "        # Pass the output of the ReLU activation through the fully connected layer\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emsize = 24\n",
    "model_cbow = CBOW(vocab_size, emsize, vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR = 5  # learning rate\n",
    "\n",
    "# Define the CrossEntropyLoss criterion. It is commonly used for multi-class classification tasks.\n",
    "# This criterion combines the softmax function and the negative log-likelihood loss.\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer using stochastic gradient descent (SGD).\n",
    "# It optimizes the parameters of the model_cbow, which are obtained by model_cbow.parameters().\n",
    "# The learning rate (lr) determines the step size for parameter updates during optimization.\n",
    "optimizer = torch.optim.SGD(model_cbow.parameters(), lr=LR)\n",
    "\n",
    "# Define a learning rate scheduler.\n",
    "# The StepLR scheduler adjusts the learning rate during training.\n",
    "# It multiplies the learning rate by gamma every step_size epochs (here, 1.0).\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbow, epoch_losses=train_model(model_cbow, dataloader_cbow, criterion, optimizer, num_epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_losses)\n",
    "plt.xlabel(\"epochs\")\n",
    "word_embeddings = model_cbow.embedding.weight.detach().cpu().numpy()\n",
    "word = 'baller'\n",
    "word_index = vocab.get_stoi()[word] # getting the index of the word in the vocab\n",
    "print(word_embeddings[word_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(word_embeddings,vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the window size for the context around the target word.\n",
    "CONTEXT_SIZE = 2\n",
    "\n",
    "# Initialize an empty list to store the (target, context) pairs.\n",
    "skip_data = []\n",
    "\n",
    "# Iterate over each word in the tokenized toy_data, while excluding the first \n",
    "# and last few words determined by the CONTEXT_SIZE.\n",
    "for i in range(CONTEXT_SIZE, len(tokenized_toy_data) - CONTEXT_SIZE):\n",
    "\n",
    "    # For a word at position i, the context comprises of words from the preceding CONTEXT_SIZE\n",
    "    # as well as from the succeeding CONTEXT_SIZE. The context words are collected in a list.\n",
    "    context = (\n",
    "        [tokenized_toy_data[i - j - 1] for j in range(CONTEXT_SIZE)]  # Preceding words\n",
    "        + [tokenized_toy_data[i + j + 1] for j in range(CONTEXT_SIZE)]  # Succeeding words\n",
    "    )\n",
    "\n",
    "    # The word at the current position i is taken as the target.\n",
    "    target = tokenized_toy_data[i]\n",
    "\n",
    "    # Append the (target, context) pair to the skip_data list.\n",
    "    skip_data.append((target, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_data_=[[(sample[0],word) for word in  sample[1]] for sample in skip_data]\n",
    "skip_data_flat= [item  for items in  skip_data_ for item in items]\n",
    "skip_data_flat[8:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    target_list, context_list = [], []\n",
    "    for _context, _target in batch:\n",
    "        \n",
    "        target_list.append(vocab[_target]) \n",
    "        context_list.append(vocab[_context])\n",
    "        \n",
    "    target_list = torch.tensor(target_list, dtype=torch.int64)\n",
    "    context_list = torch.tensor(context_list, dtype=torch.int64)\n",
    "    return target_list.to(device), context_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(skip_data_flat, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram_Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(SkipGram_Model, self).__init__()\n",
    "        # Define the embeddings layer\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim\n",
    "        )\n",
    "        \n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(in_features=embed_dim, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # Perform the forward pass\n",
    "        # Pass the input text through the embeddings layer\n",
    "        out = self.embeddings(text)\n",
    "        \n",
    "        # Pass the output of the embeddings layer through the fully connected layer\n",
    "        # Apply the ReLU activation function\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emsize = 24\n",
    "model_sg = SkipGram_Model(vocab_size, emsize).to(device)\n",
    "\n",
    "LR = 5  # learning rate\n",
    "#BATCH_SIZE = 64  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_sg.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "model_sg, epoch_losses=train_model(model_sg, dataloader, criterion, optimizer, num_epochs=400)\n",
    "\n",
    "plt.plot(epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = model_sg.embeddings.weight.detach().cpu().numpy() \n",
    "plot_embeddings(word_embeddings,vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence-to-Sequence RNN Models: Translation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_len, emb_dim, hid_dim, n_layers, dropout_prob):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_len, emb_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        #input_batch = [src len, batch size]\n",
    "        embed = self.dropout(self.embedding(input_batch))\n",
    "        embed = embed.to(device)\n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        outputs, (hidden, cell) = self.lstm(embed)\n",
    "\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = 8\n",
    "emb_dim = 10\n",
    "hid_dim=8\n",
    "n_layers=1\n",
    "dropout_prob=0.5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder_t = Encoder(vocab_len, emb_dim, hid_dim, n_layers, dropout_prob).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_batch = torch.tensor([[0,3,4,2,1]])\n",
    "# you need to transpose the input tensor as the encoder LSTM is in Sequence_first mode by default\n",
    "src_batch = src_batch.t().to(device)\n",
    "print(\"Shape of input(src) tensor:\", src_batch.shape)\n",
    "hidden_t , cell_t = encoder_t(src_batch)\n",
    "print(\"Hidden tensor from encoder:\",hidden_t ,\"\\nCell tensor from encoder:\", cell_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "\n",
    "\n",
    "        #input = [batch size]\n",
    "\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "\n",
    "        input = input.unsqueeze(0)\n",
    "        #input = [1, batch size]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch size, emb dim]\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        prediction_logit = self.fc_out(output.squeeze(0))\n",
    "        prediction = self.softmax(prediction_logit)\n",
    "        #prediction = [batch size, output dim]\n",
    "\n",
    "\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 6\n",
    "emb_dim=10\n",
    "hid_dim = 8\n",
    "n_layers=1\n",
    "dropout=0.5\n",
    "decoder_t = Decoder(output_dim, emb_dim, hid_dim, n_layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_t = torch.tensor([0]).to(device) #<bos>\n",
    "input_t.shape\n",
    "prediction, hidden, cell = decoder_t(input_t, hidden_t , cell_t)\n",
    "print(\"Prediction:\", prediction, '\\nHidden:',hidden,'\\nCell:', cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder and decoder connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#trg = [trg len, batch size]\n",
    "#teacher_forcing_ratio is probability to use teacher forcing\n",
    "#e.g. if teacher_forcing_ratio is 0.75 you use ground-truth inputs 75% of the time\n",
    "teacher_forcing_ratio = 0.5\n",
    "trg = torch.tensor([[0],[2],[3],[5],[1]]).to(device)\n",
    "\n",
    "\n",
    "batch_size = trg.shape[1]\n",
    "trg_len = trg.shape[0]\n",
    "trg_vocab_size = decoder_t.output_dim\n",
    "\n",
    "#tensor to store decoder outputs\n",
    "outputs_t = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "\n",
    "#send to device\n",
    "\n",
    "hidden_t = hidden_t.to(device)\n",
    "cell_t = cell_t.to(device)\n",
    "\n",
    "\n",
    "#first input to the decoder is the <bos> tokens\n",
    "input = trg[0,:]\n",
    "\n",
    "\n",
    "for t in range(1, trg_len):\n",
    "\n",
    "    #you loop through the trg len and generate tokens\n",
    "    #decoder receives previous generated token, cell and hidden\n",
    "    # decoder outputs it prediction(probablity distribution for the next token) and updates hidden and cell\n",
    "    output_t, hidden_t, cell_t = decoder_t(input, hidden_t, cell_t)\n",
    "\n",
    "    #place predictions in a tensor holding predictions for each token\n",
    "    outputs_t[t] = output_t\n",
    "\n",
    "    #decide if you are going to use teacher forcing or not\n",
    "    teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "    #get the highest predicted token from your predictions\n",
    "    top1 = output_t.argmax(1)\n",
    "\n",
    "\n",
    "    #if teacher forcing, use actual next token as next input\n",
    "    #if not, use predicted token\n",
    "    #input = trg[t] if teacher_force else top1\n",
    "    input = trg[t] if teacher_force else top1\n",
    "\n",
    "print(outputs_t,outputs_t.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that you need to get the argmax from the second dimension as **outputs** is an array of **output** tensors\n",
    "pred_tokens = outputs_t.argmax(2)\n",
    "print(pred_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Connect encoder and decoder to create seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device,trg_vocab):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.trg_vocab = trg_vocab\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 you use ground-truth inputs 75% of the time\n",
    "\n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        hidden = hidden.to(device)\n",
    "        cell = cell.to(device)\n",
    "\n",
    "\n",
    "        #first input to the decoder is the <bos> tokens\n",
    "        input = trg[0,:]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "\n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "\n",
    "            #decide if you are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            #get the highest predicted token from your predictions\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "\n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            #input = trg[t] if teacher_force else top1\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Wrap iterator with tqdm for progress logging\n",
    "    train_iterator = tqdm(iterator, desc=\"Training\", leave=False)\n",
    "\n",
    "    for i, (src,trg) in enumerate(iterator):\n",
    "\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "\n",
    "        trg = trg[1:].contiguous().view(-1)\n",
    "\n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update tqdm progress bar with the current loss\n",
    "        train_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(list(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Wrap iterator with tqdm for progress logging\n",
    "    valid_iterator = tqdm(iterator, desc=\"Training\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (src,trg) in enumerate(iterator):\n",
    "\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "\n",
    "            trg = trg[1:].contiguous().view(-1)\n",
    "\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            # Update tqdm progress bar with the current loss\n",
    "            valid_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(list(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Multi30K_de_en_dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, trg = next(iter(train_dataloader))\n",
    "src,trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_itr = iter(train_dataloader)\n",
    "# moving forward in the dataset to reach sequences of longer length for illustration purpose. (Remember the dataset is sorted on sequence len for optimal padding)\n",
    "for n in range(1000):\n",
    "    german, english= next(data_itr)\n",
    "\n",
    "for n in range(3):\n",
    "    german, english=next(data_itr)\n",
    "    german=german.T\n",
    "    english=english.T\n",
    "    print(\"________________\")\n",
    "    print(\"german\")\n",
    "    for g in german:\n",
    "        print(index_to_german(g))\n",
    "    print(\"________________\")\n",
    "    print(\"english\")\n",
    "    for e in english:\n",
    "        print(index_to_eng(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_transform['de'])\n",
    "OUTPUT_DIM = len(vocab_transform['en'])\n",
    "ENC_EMB_DIM = 128 #256\n",
    "DEC_EMB_DIM = 128 #256\n",
    "HID_DIM = 256 #512\n",
    "N_LAYERS = 1 #2\n",
    "ENC_DROPOUT = 0.3 #0.5\n",
    "DEC_DROPOUT = 0.3 #0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device,trg_vocab = vocab_transform['en']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "PAD_IDX = vocab_transform['en'].get_stoi()['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "N_EPOCHS = 3 #run the training for at least 5 epochs\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "best_train_loss = float('inf')\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "train_PPLs = []\n",
    "valid_PPLs = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "    train_ppl = math.exp(train_loss)\n",
    "    valid_loss = evaluate(model, valid_dataloader, criterion)\n",
    "    valid_ppl = math.exp(valid_loss)\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'RNN-TR-model.pt')\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_PPLs.append(train_ppl)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_PPLs.append(valid_ppl)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {valid_ppl:7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
