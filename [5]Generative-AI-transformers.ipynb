{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI - Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention and Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "\n",
    "from Levenshtein import distance\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device for training\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "split = 'train'\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 3e-4\n",
    "batch_size = 64\n",
    "max_iters = 5000              # Maximum training iterations\n",
    "eval_interval = 200           # Evaluate model every 'eval_interval' iterations in the training loop\n",
    "eval_iters = 100              # When evaluating, approximate loss using 'eval_iters' batches\n",
    "\n",
    "# Architecture parameters\n",
    "max_vocab_size = 256          # Maximum vocabulary size\n",
    "vocab_size = max_vocab_size   # Real vocabulary size (e.g. BPE has a variable length, so it can be less than 'max_vocab_size')\n",
    "block_size = 16               # Context length for predictions\n",
    "n_embd = 32                   # Embedding size\n",
    "num_heads = 2                 # Number of head in multi-headed attention\n",
    "n_layer = 2                   # Number of Blocks\n",
    "ff_scale_factor = 4           # Note: The '4' magic number is from the paper: In equation 2 uses d_model=512, but d_ff=2048\n",
    "dropout = 0.0                 # Normalization using dropout# 10.788929 M parameters\n",
    "\n",
    "head_size = n_embd // num_heads\n",
    "assert (num_heads * head_size) == n_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embdings(my_embdings,name,vocab):\n",
    "\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "  # Plot the data points\n",
    "  ax.scatter(my_embdings[:,0], my_embdings[:,1], my_embdings[:,2])\n",
    "\n",
    "  # Label the points\n",
    "  for j, label in enumerate(name):\n",
    "      i=vocab.get_stoi()[label]\n",
    "      ax.text(my_embdings[j,0], my_embdings[j,1], my_embdings[j,2], label)\n",
    "\n",
    "  # Set axis labels\n",
    "  ax.set_xlabel('X Label')\n",
    "  ax.set_ylabel('Y Label')\n",
    "  ax.set_zlabel('Z Label')\n",
    "\n",
    "  # Show the plot\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    'le': 'the'\n",
    "    , 'chat': 'cat'\n",
    "    , 'est': 'is'\n",
    "    , 'sous': 'under'\n",
    "    , 'la': 'the'\n",
    "    , 'table': 'table'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split a sentence into tokens (words)\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    This function takes a string of text as input and returns a list of words (tokens).\n",
    "    It uses the split method, which by default splits on any whitespace, to tokenize the text.\n",
    "    \"\"\"\n",
    "    return text.split()  # Split the input text on whitespace and return the list of tokens\n",
    "\n",
    "# Function to translate a sentence from source to target language word by word\n",
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    This function translates a sentence by looking up each word's translation in a predefined dictionary.\n",
    "    It assumes that every word in the sentence is a key in the dictionary.\n",
    "    \"\"\"\n",
    "    out = ''  # Initialize the output string\n",
    "    for token in tokenize(sentence):  # Tokenize the sentence into words\n",
    "        # Append the translated word to the output string\n",
    "        # This line assumes the dictionary contains a translation for every word in the input\n",
    "        out += dictionary[token] + ' '\n",
    "    return out.strip()  # Return the translated sentence, stripping any extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"le chat est sous la table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the closest key in the dictionary to the given query word\n",
    "def find_closest_key(query):\n",
    "    \"\"\"\n",
    "    The function computes the Levenshtein distance between the query and each key in the dictionary.\n",
    "    The Levenshtein distance is a measure of the number of single-character edits required to change one word into the other.\n",
    "    \"\"\"\n",
    "    closest_key, min_dist = None, float('inf')  # Initialize the closest key and minimum distance to infinity\n",
    "    for key in dictionary.keys():\n",
    "        dist = distance(query, key)  # Calculate the Levenshtein distance to the current key\n",
    "        if dist < min_dist:  # If the current distance is less than the previously found minimum\n",
    "            min_dist, closest_key = dist, key  # Update the minimum distance and the closest key\n",
    "    return closest_key  # Return the closest key found\n",
    "\n",
    "# Function to translate a sentence from source to target language using the dictionary\n",
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    This function tokenizes the input sentence into words and finds the closest translation for each word.\n",
    "    It constructs the translated sentence by appending the translated words together.\n",
    "    \"\"\"\n",
    "    out = ''  # Initialize the output string\n",
    "    for query in tokenize(sentence):  # Tokenize the sentence into words\n",
    "        key = find_closest_key(query)  # Find the closest key in the dictionary for each word\n",
    "        out += dictionary[key] + ' '  # Append the translation of the closest key to the output string\n",
    "    return out.strip()  # Return the translated sentence, stripping any extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and sort the input vocabulary from the dictionary's keys\n",
    "vocabulary_in = sorted(list(set(dictionary.keys())))\n",
    "# Display the size and the sorted vocabulary for the input language\n",
    "print(f\"Vocabulary input ({len(vocabulary_in)}): {vocabulary_in}\")\n",
    "\n",
    "# Create and sort the output vocabulary from the dictionary's values\n",
    "vocabulary_out = sorted(list(set(dictionary.values())))\n",
    "# Display the size and the sorted vocabulary for the output language\n",
    "print(f\"Vocabulary output ({len(vocabulary_out)}): {vocabulary_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a list of vocabulary words into one-hot encoded vectors\n",
    "def encode_one_hot(vocabulary):\n",
    "    vocabulary_size = len(vocabulary)  # Get the size of the vocabulary\n",
    "    one_hot = dict()  # Initialize a dictionary to hold our one-hot encodings\n",
    "    LEN = len(vocabulary)  # The length of each one-hot encoded vector will be equal to the vocabulary size\n",
    "    \n",
    "    # Iterate over the vocabulary to create a one-hot encoded vector for each word\n",
    "    for i, key in enumerate(vocabulary):\n",
    "        one_hot_vector = torch.zeros(LEN)  # Start with a vector of zeros\n",
    "        one_hot_vector[i] = 1  # Set the i-th position to 1 for the current word\n",
    "        one_hot[key] = one_hot_vector  # Map the word to its one-hot encoded vector\n",
    "        print(f\"{key}\\t: {one_hot[key]}\")  # Print each word and its encoded vector\n",
    "    \n",
    "    return one_hot  # Return the dictionary of words and their one-hot encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_one_hot(one_hot, vector):\n",
    "    \"\"\" \n",
    "    Decode a one-hot encoded vector to find the best matching token in the vocabulary.\n",
    "    \"\"\"\n",
    "    best_key, best_cosine_sim = None, 0\n",
    "    for k, v in one_hot.items():  # Iterate over the one-hot encoded vocabulary\n",
    "        cosine_sim = torch.dot(vector, v)  # Calculate dot product (cosine similarity)\n",
    "        if cosine_sim > best_cosine_sim:  # If this is the best similarity we've found\n",
    "            best_cosine_sim, best_key = cosine_sim, k  # Update the best similarity and token\n",
    "    return best_key  # Return the token corresponding to the one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    \"\"\" \n",
    "    Translate a sentence using matrix multiplication, treating the dictionaries as matrices.\n",
    "    \"\"\"\n",
    "    sentence_out = ''  # Initialize the output sentence\n",
    "    for token_in in tokenize(sentence):  # Tokenize the input sentence\n",
    "        q = one_hot_in[token_in]  # Find the one-hot vector for the token\n",
    "        out = q @ K.T @ V  # Multiply with the input and output matrices to find the translation\n",
    "        token_out = decode_one_hot(one_hot_out, out)  # Decode the output one-hot vector to a token\n",
    "        sentence_out += token_out + ' '  # Append the translated token to the output sentence\n",
    "    return sentence_out.strip()  # Return the translated sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    Translate a sentence using the attention mechanism represented by the K and V matrices.\n",
    "    The softmax function is used to calculate a weighted sum of the V vectors, focusing on the most relevant vector for translation.\n",
    "    \"\"\"\n",
    "    sentence_out = ''  # Initialize the output sentence\n",
    "    for token_in in tokenize(sentence):  # Tokenize the input sentence\n",
    "        q = one_hot_in[token_in]  # Get the one-hot vector for the current token\n",
    "        # Apply softmax to the scaled dot product of q and K.T, then multiply by V\n",
    "        # This selects the most relevant translation vector from V\n",
    "        out = torch.softmax(q @ K.T, dim=0) @ V\n",
    "        token_out = decode_one_hot(one_hot_out, out)  # Decode the output vector to a token\n",
    "        sentence_out += token_out + ' '  # Append the translated token to the output sentence\n",
    "    return sentence_out.strip()  # Return the translated sentence\n",
    "\n",
    "# Test the translate function\n",
    "translate(\"le chat est sous la table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention class (create self-attention heads from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" Self attention head. This class implements a self-attention mechanism\n",
    "        which is a key component of transformer-based neural network architectures. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()  # Initialize the superclass (nn.Module)\n",
    "        # Embedding layer to convert input token indices to vectors of fixed size (n_embd)\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        # Linear layers to compute the queries, keys, and values from the embeddings\n",
    "        self.key = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.query = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.value = nn.Linear(n_embd, n_embd, bias=False)\n",
    "\n",
    "    def attention(self, x):\n",
    "        embedded_x = self.embedding(x)\n",
    "        k = self.key(embedded_x)\n",
    "        q = self.query(embedded_x)\n",
    "        v = self.value(embedded_x)\n",
    "        # Attention score\n",
    "        w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5   # Query * Keys / normalization\n",
    "        w = F.softmax(w, dim=-1)  # Do a softmax across the last dimesion\n",
    "        return embedded_x,k,q,v,w\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded_x = self.embedding(x)\n",
    "        k = self.key(embedded_x)\n",
    "        q = self.query(embedded_x)\n",
    "        v = self.value(embedded_x)\n",
    "        # Attention score\n",
    "        w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5   # Query * Keys / normalization\n",
    "        w = F.softmax(w, dim=-1)  # Do a softmax across the last dimesion\n",
    "        # Add weighted values\n",
    "        out = w @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding module injects some information about the relative or absolute position of the tokens in the sequence.\"\"\"\n",
    "    def __init__(self, n_embd, vocab_size, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Initialize a buffer for the positional encodings (not a parameter, so it's not updated during training)\n",
    "        pe = torch.zeros(vocab_size, n_embd)\n",
    "        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
    "        # Calculate the positional encodings once in log space\n",
    "        pe = torch.cat((torch.cos(2 * 3.14 * position / 25), torch.sin(2 * 3.14 * position / 25), torch.sin(2 * 3.14 * position / 5)), 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to each embedding vector assuming x is (seq_len, batch_size, n_embd)\n",
    "        # Note: 'pe' is a registered buffer and does not require gradients\n",
    "        pos = x + self.pe[:x.size(0), :]\n",
    "        return pos\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"Self attention head.\"\"\"\n",
    "    def __init__(self, n_embd, vocab_size):\n",
    "        super().__init__()\n",
    "        # An embedding layer that converts input data (token indices) into dense vectors of fixed size\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        # The positional encoding layer\n",
    "        self.pos_encoder = PositionalEncoding(n_embd, vocab_size)\n",
    "        # Layers to transform the position-encoded embeddings into queries, keys, and values\n",
    "        self.key = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.query = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.value = nn.Linear(n_embd, n_embd, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the embedding layer to get fixed size dense embeddings\n",
    "        embedded_x = self.embedding(x)\n",
    "        # Pass the embeddings through the positional encoder\n",
    "        p_encoded_x = self.pos_encoder(embedded_x)\n",
    "        # Generate queries, keys, and values for the attention\n",
    "        k = self.key(p_encoded_x)\n",
    "        q = self.query(p_encoded_x)\n",
    "        v = self.value(p_encoded_x)\n",
    "        # Calculate the attention scores as the dot product of queries and keys\n",
    "        w = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5  # Query * Keys / normalization\n",
    "        # Apply the softmax function to the attention scores to get probabilities\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        # Multiply the attention weights with the values to get the output\n",
    "        out = w @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Head class with embedding dimension and vocabulary size as parameters\n",
    "transformer = Head(n_embd, vocab_size)\n",
    "\n",
    "# Pass the input data through the transformer model to obtain the output\n",
    "# This process includes embedding the input, adding positional encodings, and applying self-attention\n",
    "out = transformer(input_data)\n",
    "\n",
    "# Print the shape of the output tensor\n",
    "# The shape will provide insight into how the data has been transformed through the model\n",
    "print(\"Output shape:\", out.shape)\n",
    "\n",
    "# Display the output tensor itself\n",
    "# This output represents the transformed data after applying the embedding, positional encoding, and self-attention mechanisms\n",
    "print(\"Output:\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiHead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Embedding dimension\n",
    "embed_dim =4\n",
    "# Number of attention heads\n",
    "num_heads = 2\n",
    "print(\"should be zero:\",embed_dim %num_heads)\n",
    "# Initialize MultiheadAttention\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads,batch_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 10 # Sequence length\n",
    "batch_size = 5 # Batch size\n",
    "query = torch.rand((seq_length, batch_size, embed_dim))\n",
    "key = torch.rand((seq_length, batch_size, embed_dim))\n",
    "value = torch.rand((seq_length, batch_size, embed_dim))\n",
    "# Perform multi-head attention\n",
    "attn_output, _= multihead_attn(query, key, value)\n",
    "print(\"Attention Output Shape:\", attn_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Transformers for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(COST,ACC):\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(COST, color=color)\n",
    "    ax1.set_xlabel('epoch', color=color)\n",
    "    ax1.set_ylabel('total loss', color=color)\n",
    "    ax1.tick_params(axis='y', color=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('accuracy', color=color)  # you already handled the x-label with ax1\n",
    "    ax2.plot(ACC, color=color)\n",
    "    ax2.tick_params(axis='y', color=color)\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embdings(my_embdings,name,vocab):\n",
    "  \n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "  # Plot the data points\n",
    "  ax.scatter(my_embdings[:,0], my_embdings[:,1], my_embdings[:,2])\n",
    "\n",
    "  # Label the points\n",
    "  for j, label in enumerate(name):\n",
    "      i=vocab.get_stoi()[label]\n",
    "      ax.text(my_embdings[j,0], my_embdings[j,1], my_embdings[j,2], label)\n",
    "\n",
    "  # Set axis labels\n",
    "  ax.set_xlabel('X Label')\n",
    "  ax.set_ylabel('Y Label')\n",
    "  ax.set_zlabel('Z Label')\n",
    "\n",
    "  # Show the plot\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_tras(words, model):\n",
    "    # Tokenize the input words using a tokenizer function\n",
    "    tokens = tokenizer(words)\n",
    "\n",
    "    # Define the model's embedding dimension (d_model)\n",
    "    d_model = 100\n",
    "\n",
    "    # Convert the input words to a PyTorch tensor and move it to the specified device\n",
    "    x = torch.tensor(text_pipeline(words)).unsqueeze(0).to(device)\n",
    "\n",
    "    # Apply the model's embedding layer and scale the embeddings by sqrt(d_model)\n",
    "    x_ = model.emb(x) * math.sqrt(d_model)\n",
    "\n",
    "    # Apply the model's positional encoder to the embeddings\n",
    "    x = model.pos_encoder(x_)\n",
    "\n",
    "    # Extract projection weights for query, key, and value from the model's state_dict\n",
    "    q_proj_weight = model.state_dict()['transformer_encoder.layers.0.self_attn.in_proj_weight'][0:embed_dim].t()\n",
    "    k_proj_weight = model.state_dict()['transformer_encoder.layers.0.self_attn.in_proj_weight'][embed_dim:2*embed_dim].t()\n",
    "    v_proj_weight = model.state_dict()['transformer_encoder.layers.0.self_attn.in_proj_weight'][2*embed_dim:3*embed_dim].t()\n",
    "\n",
    "    # Calculate query (Q), key (K), and value (V) matrices\n",
    "    Q = (x @ q_proj_weight).squeeze(0)\n",
    "    K = (x @ k_proj_weight).squeeze(0)\n",
    "    V = (x @ v_proj_weight).squeeze(0)\n",
    "\n",
    "    # Calculate attention scores using dot-product attention\n",
    "    scores = Q @ K.T\n",
    "\n",
    "    # Set row and column labels for the attention matrix\n",
    "    row_labels = tokens\n",
    "    col_labels = row_labels\n",
    "\n",
    "    # Create a heatmap of the attention scores\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(scores.cpu().detach().numpy())\n",
    "    plt.yticks(range(len(row_labels)), row_labels)\n",
    "    plt.xticks(range(len(col_labels)), col_labels, rotation=90)\n",
    "    plt.title(\"Dot-Product Attention\")\n",
    "    plt.show()\n",
    "\n",
    "    # Apply softmax to the attention scores and create a heatmap\n",
    "    att = nn.Softmax(dim=1)(scores)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(att.cpu().detach().numpy())\n",
    "    plt.yticks(range(len(row_labels)), row_labels)\n",
    "    plt.xticks(range(len(col_labels)), col_labels, rotation=90)\n",
    "    plt.title(\"Scaled Dot-Product Attention\")\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate the attention head by multiplying softmax scores with values (V)\n",
    "    head = nn.Softmax(dim=1)(scores) @ V\n",
    "\n",
    "    # Visualize the embeddings and attention heads using t-SNE\n",
    "    tsne(x_, tokens, title=\"Embeddings\")\n",
    "    tsne(head, tokens, title=\"Attention Heads\")\n",
    "\n",
    "\n",
    "def tsne(embeddings, tokens, title=\"Embeddings\"):\n",
    "    # Initialize t-SNE with 2 components and a fixed random state\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "    # Fit t-SNE to the embeddings (converting from GPU if necessary)\n",
    "    tsne_result = tsne.fit_transform(embeddings.squeeze(0).cpu().detach().numpy())\n",
    "\n",
    "    # Create a scatter plot of the t-SNE results\n",
    "    plt.scatter(tsne_result[:, 0], tsne_result[:, 1])\n",
    "\n",
    "    # Set a title for the plot\n",
    "    plt.title(title)\n",
    "\n",
    "    # Add labels for each point in the scatter plot\n",
    "    for j, label in enumerate(tokens):\n",
    "        # Place the label text at the corresponding t-SNE coordinates\n",
    "        plt.text(tsne_result[j, 0], tsne_result[j, 1], label)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_file(lst, filename):\n",
    "    \"\"\"\n",
    "    Save a list to a file using pickle serialization.\n",
    "\n",
    "    Parameters:\n",
    "        lst (list): The list to be saved.\n",
    "        filename (str): The name of the file to save the list to.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(lst, file)\n",
    "\n",
    "def load_list_from_file(filename):\n",
    "    \"\"\"\n",
    "    Load a list from a file using pickle deserialization.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): The name of the file to load the list from.\n",
    "\n",
    "    Returns:\n",
    "        list: The loaded list.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        loaded_list = pickle.load(file)\n",
    "    return loaded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, vocab_size=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(vocab_size, d_model)\n",
    "        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float()\n",
    "            * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGPTModel(nn.Module):\n",
    "    def __init__(self, embed_size,vocab_size, num_heads, num_layers, max_seq_len=500,dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.init_weights()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = PositionalEncoding(embed_size, dropout=dropout)\n",
    "\n",
    "        print( embed_size )\n",
    "\n",
    "\n",
    "        # Remaining layers are part of the TransformerDecoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        self.embed_size = embed_size\n",
    "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def init_weights(self):\n",
    "      for p in self.parameters():\n",
    "          if p.dim() > 1:\n",
    "              nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def create_mask(src,device=DEVICE):\n",
    "        src_seq_len = src.shape[0]\n",
    "        src_mask = nn.Transformer.generate_square_subsequent_mask(src_seq_len)\n",
    "        src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "        return src_mask,src_padding_mask\n",
    "\n",
    "    def decoder(self, x,src_mask):\n",
    "        seq_length = x.size(0)\n",
    "\n",
    "        # Add positional embeddings to the input embeddings\n",
    "        x = self.embed(x)* math.sqrt(self.embed_size)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask, src_padding_mask = create_mask(x)\n",
    "\n",
    "        output = self.transformer_encoder(x, src_mask)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    def forward(self,x,src_mask=None,key_padding_mask=None):\n",
    "\n",
    "        seq_length = x.size(0)\n",
    "\n",
    "        # Add positional embeddings to the input embeddings\n",
    "        x = self.embed(x)* math.sqrt(self.embed_size) #src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask, src_padding_mask = create_mask(x)\n",
    "\n",
    "        output = self.transformer_encoder(x, src_mask,key_padding_mask)\n",
    "        x = self.lm_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_prompt(prompt, block_size=BLOCK_SIZE):\n",
    "    # Handle None prompt\n",
    "    while prompt is None:\n",
    "        prompt = input(\"Sorry, prompt cannot be empty. Please enter a valid prompt: \")\n",
    "\n",
    "    tokens = tokenizer(prompt)\n",
    "    number_of_tokens = len(tokens)\n",
    "\n",
    "    # Handle long prompts\n",
    "    if number_of_tokens > block_size:\n",
    "        tokens = tokens[-block_size:]  # Keep last block_size characters\n",
    "\n",
    "    prompt_indices = vocab(tokens)\n",
    "    prompt_encoded = torch.tensor(prompt_indices, dtype=torch.int64).reshape(-1, 1)\n",
    "    return prompt_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining BERT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTCSVDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        self.data = pd.read_csv(filename)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        try:\n",
    "            \n",
    "            bert_input = torch.tensor(json.loads(row['BERT Input']), dtype=torch.long)\n",
    "            bert_label = torch.tensor(json.loads(row['BERT Label']), dtype=torch.long)\n",
    "            segment_label = torch.tensor([int(x) for x in row['Segment Label'].split(',')], dtype=torch.long)\n",
    "            is_next = torch.tensor(row['Is Next'], dtype=torch.long)\n",
    "            original_text = row['Original Text']  # If you want to use it\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON for row {idx}: {e}\")\n",
    "            print(\"BERT Input:\", row['BERT Input'])\n",
    "            print(\"BERT Label:\", row['BERT Label'])\n",
    "            # Handle the error, e.g., by skipping this row or using default values\n",
    "            return None  # or some default values\n",
    "        \n",
    "        return bert_input, bert_label, segment_label, is_next  # Include original_text if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "def collate_batch(batch):\n",
    "    bert_inputs_batch, bert_labels_batch, segment_labels_batch, is_nexts_batch = [], [], [], []\n",
    "\n",
    "    for bert_input, bert_label, segment_label, is_next in batch:\n",
    "        # Convert each sequence to a tensor and append to the respective list\n",
    "        bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
    "        bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
    "        segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n",
    "        is_nexts_batch.append(is_next)\n",
    "\n",
    "    # Pad the sequences in the batch\n",
    "    bert_inputs_final = pad_sequence(bert_inputs_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    bert_labels_final = pad_sequence(bert_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    segment_labels_final = pad_sequence(segment_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    is_nexts_batch = torch.tensor(is_nexts_batch, dtype=torch.long)\n",
    "\n",
    "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataset_path = './bert_dataset/bert_train_data.csv'\n",
    "test_dataset_path = './bert_dataset/bert_test_data.csv'\n",
    "\n",
    "train_dataset = BERTCSVDataset(train_dataset_path)\n",
    "test_dataset = BERTCSVDataset(test_dataset_path)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 10\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Define the PositionalEncoding class as a PyTorch module for adding positional information to token embeddings\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a positional encoding matrix as per the Transformer paper's formula\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor):\n",
    "        # Apply the positional encodings to the input token embeddings\n",
    "\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding (nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size ,dropout=0.1,train=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = TokenEmbedding( vocab_size,emb_size )\n",
    "        self.positional_encoding = PositionalEncoding(emb_size,dropout)\n",
    "        self.segment_embedding = nn.Embedding(3, emb_size)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, bert_inputs, segment_labels=False):\n",
    "        my_embeddings=self.token_embedding(bert_inputs)\n",
    "        if self.train:\n",
    "          x = self.dropout(my_embeddings + self.positional_encoding(my_embeddings) + self.segment_embedding(segment_labels))\n",
    "        else:\n",
    "          x = my_embeddings + self.positional_encoding(my_embeddings)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE=147161\n",
    "batch = 2\n",
    "count = 0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# load sample batches from dataloader\n",
    "for batch in train_dataloader:\n",
    "    bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer with the BERT model's vocabulary\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "def predict_nsp(sentence1, sentence2, model, tokenizer):\n",
    "    # Tokenize sentences with special tokens\n",
    "    tokens = tokenizer.encode_plus(sentence1, sentence2, return_tensors=\"pt\")\n",
    "    tokens_tensor = tokens[\"input_ids\"].to(device)\n",
    "    segment_tensor = tokens[\"token_type_ids\"].to(device)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        # Assuming the model returns NSP predictions first\n",
    "        nsp_prediction, _ = model(tokens_tensor, segment_tensor)\n",
    "        # Select the first element (first sequence) of the logits tensor\n",
    "        first_logits = nsp_prediction[0].unsqueeze(0)  # Adds an extra dimension, making it [1, 2]\n",
    "        logits = torch.softmax(first_logits, dim=1)\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Interpret the prediction\n",
    "    return \"Second sentence follows the first\" if prediction == 1 else \"Second sentence does not follow the first\"\n",
    "\n",
    "# Example usage\n",
    "sentence1 = \"The cat sat on the mat.\"\n",
    "sentence2 = \"It was a sunny day\"\n",
    "\n",
    "print(predict_nsp(sentence1, sentence2, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mlm(sentence, model, tokenizer):\n",
    "    # Tokenize the input sentence and convert to token IDs, including special tokens\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    tokens_tensor = inputs.input_ids\n",
    "\n",
    "    # Create dummy segment labels filled with zeros, assuming it's needed by your model\n",
    "    segment_labels = torch.zeros_like(tokens_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the model, now correctly handling the output tuple\n",
    "        output_tuple = model(tokens_tensor, segment_labels)\n",
    "\n",
    "        # Assuming the second element of the tuple contains the MLM logits\n",
    "        predictions = output_tuple[1]  # Adjusted based on your model's output\n",
    "\n",
    "        # Identify the position of the [MASK] token\n",
    "        mask_token_index = (tokens_tensor == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Get the predicted index for the [MASK] token from the MLM logits\n",
    "        predicted_index = torch.argmax(predictions[0, mask_token_index.item(), :], dim=-1)\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index.item()])[0]\n",
    "\n",
    "        # Replace [MASK] in the original sentence with the predicted token\n",
    "        predicted_sentence = sentence.replace(tokenizer.mask_token, predicted_token, 1)\n",
    "\n",
    "    return predicted_sentence\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sentence = \"The cat sat on the [MASK].\"\n",
    "print(predict_mlm(sentence, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Text Processing for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for label, data_sample in data_iter:\n",
    "        yield tokenizer(data_sample)\n",
    "\n",
    "# Define special symbols and indices\n",
    "PAD_IDX,CLS_IDX, SEP_IDX,  MASK_IDX,UNK_IDX= 0, 1, 2, 3, 4\n",
    "\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['[PAD]','[CLS]', '[SEP]','[MASK]','[UNK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data splits\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "all_data_iter = chain(train_iter, test_iter)\n",
    "#check tokenizer\n",
    "# list(yield_tokens(all_data_iter))[5][:20]\n",
    "fifth_item_tokens = next(islice(yield_tokens(all_data_iter), 5, None))\n",
    "print(fifth_item_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create vocab : vocab is only built using train data\n",
    "vocab=build_vocab_from_iterator(yield_tokens(all_data_iter),specials=special_symbols,special_first=True)\n",
    "\n",
    "vocab.set_default_index(UNK_IDX)\n",
    "VOCAB_SIZE=len(vocab)\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_index=lambda text: [vocab(token) for token in tokenizer(text)]\n",
    "index_to_en = lambda seq_en: \" \".join([vocab.get_itos()[index] for index in seq_en])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_en = [0, 1, 2, 3, 4, 5, 6]  # Example input sequence\n",
    "english_sentence = index_to_en(seq_en)\n",
    "seq2=[6,16,26131]\n",
    "english_sentence = index_to_en(seq2)\n",
    "\n",
    "print(english_sentence)\n",
    "\n",
    "text = \"I've seen R-rated films with male nudity. Nowhere, because they don't exist.\"  # Example input text\n",
    "text_to_index = lambda text: [vocab[token] for token in tokenizer(text)]\n",
    "index_sequence = text_to_index(text)\n",
    "\n",
    "print(index_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text masking\n",
    "def bernoulli_true_false(p):\n",
    "    # Create a Bernoulli distribution with probability p\n",
    "    bernoulli_dist = torch.distributions.Bernoulli(torch.tensor([p]))\n",
    "    # Sample from this distribution and convert 1 to True and 0 to False\n",
    "    return bernoulli_dist.sample().item() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Masking(token):\n",
    "    # Decide whether to mask this token (20% chance)\n",
    "    mask = bernoulli_true_false(0.2)\n",
    "\n",
    "    # If mask is False, immediately return with '[PAD]' label\n",
    "    if not mask:\n",
    "        return token, '[PAD]'\n",
    "\n",
    "    # If mask is True, proceed with further operations\n",
    "    # Randomly decide on an operation (50% chance each)\n",
    "    random_opp = bernoulli_true_false(0.5)\n",
    "    random_swich = bernoulli_true_false(0.5)\n",
    "\n",
    "    # Case 1: If mask, random_opp, and random_swich are True\n",
    "    if mask and random_opp and random_swich:\n",
    "        # Replace the token with '[MASK]' and set label to a random token\n",
    "        mask_label = index_to_en(torch.randint(0, VOCAB_SIZE, (1,)))\n",
    "        token_ = '[MASK]'\n",
    "\n",
    "    # Case 2: If mask and random_opp are True, but random_swich is False\n",
    "    elif mask and random_opp and not random_swich:\n",
    "        # Leave the token unchanged and set label to the same token\n",
    "        token_ = token\n",
    "        mask_label = token\n",
    "\n",
    "    # Case 3: If mask is True, but random_opp is False\n",
    "    else:\n",
    "        # Replace the token with '[MASK]' and set label to the original token\n",
    "        token_ = '[MASK]'\n",
    "        mask_label = token\n",
    "\n",
    "    return token_, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_mlm(tokens, include_raw_tokens=False):\n",
    "    \"\"\"\n",
    "    Prepares tokenized text for BERT's Masked Language Model (MLM) training.\n",
    "\n",
    "    \"\"\"\n",
    "    bert_input = []  # List to store sentences processed for BERT's MLM\n",
    "    bert_label = []  # List to store labels for each token (mask, random, or unchanged)\n",
    "    raw_tokens_list = []  # List to store raw tokens if needed\n",
    "    current_bert_input = []\n",
    "    current_bert_label = []\n",
    "    current_raw_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        # Apply BERT's MLM masking strategy to the token\n",
    "        masked_token, mask_label = Masking(token)\n",
    "\n",
    "        # Append the processed token and its label to the current sentence and label list\n",
    "        current_bert_input.append(masked_token)\n",
    "        current_bert_label.append(mask_label)\n",
    "\n",
    "        # If raw tokens are to be included, append the original token to the current raw tokens list\n",
    "        if include_raw_tokens:\n",
    "            current_raw_tokens.append(token)\n",
    "\n",
    "        # Check if the token is a sentence delimiter (., ?, !)\n",
    "        if token in ['.', '?', '!']:\n",
    "            # If current sentence has more than two tokens, consider it a valid sentence\n",
    "            if len(current_bert_input) > 2:\n",
    "                bert_input.append(current_bert_input)\n",
    "                bert_label.append(current_bert_label)\n",
    "                # If including raw tokens, add the current list of raw tokens to the raw tokens list\n",
    "                if include_raw_tokens:\n",
    "                    raw_tokens_list.append(current_raw_tokens)\n",
    "\n",
    "                # Reset the lists for the next sentence\n",
    "                current_bert_input = []\n",
    "                current_bert_label = []\n",
    "                current_raw_tokens = []\n",
    "            else:\n",
    "                # If the current sentence is too short, discard it and reset lists\n",
    "                current_bert_input = []\n",
    "                current_bert_label = []\n",
    "                current_raw_tokens = []\n",
    "\n",
    "    # Add any remaining tokens as a sentence if there are any\n",
    "    if current_bert_input:\n",
    "        bert_input.append(current_bert_input)\n",
    "        bert_label.append(current_bert_label)\n",
    "        if include_raw_tokens:\n",
    "            raw_tokens_list.append(current_raw_tokens)\n",
    "\n",
    "    # Return the prepared lists for BERT's MLM training\n",
    "    return (bert_input, bert_label, raw_tokens_list) if include_raw_tokens else (bert_input, bert_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_nsp(input_sentences, input_masked_labels):\n",
    "    \"\"\"\n",
    "    Prepares data for Next Sentence Prediction (NSP) task in BERT training.\n",
    "\n",
    "    Args:\n",
    "    input_sentences (list): List of tokenized sentences.\n",
    "    input_masked_labels (list): Corresponding list of masked labels for the sentences.\n",
    "\n",
    "    Returns:\n",
    "    bert_input (list): List of sentence pairs for BERT input.\n",
    "    bert_label (list): List of masked labels for the sentence pairs.\n",
    "    is_next (list): Binary label list where 1 indicates 'next sentence' and 0 indicates 'not next sentence'.\n",
    "    \"\"\"\n",
    "    if len(input_sentences) < 2:\n",
    "       raise ValueError(\"must have two same number of items.\")\n",
    "\n",
    "\n",
    "    # Verify that both input lists are of the same length and have a sufficient number of sentences\n",
    "    if len(input_sentences) != len(input_masked_labels):\n",
    "        raise ValueError(\"Both lists must have the same number of items.\")\n",
    "\n",
    "    bert_input = []\n",
    "    bert_label = []\n",
    "    is_next = []\n",
    "\n",
    "    available_indices = list(range(len(input_sentences)))\n",
    "\n",
    "    while len(available_indices) >= 2:\n",
    "        if random.random() < 0.5:\n",
    "            # Choose two consecutive sentences to simulate the 'next sentence' scenario\n",
    "            index = random.choice(available_indices[:-1])  # Exclude the last index\n",
    "            # append list and add  '[CLS]' and  '[SEP]' tokens\n",
    "            bert_input.append([['[CLS]']+input_sentences[index]+ ['[SEP]'],input_sentences[index + 1]+ ['[SEP]']])\n",
    "            bert_label.append([['[PAD]']+input_masked_labels[index]+['[PAD]'], input_masked_labels[index + 1]+ ['[PAD]']])\n",
    "            is_next.append(1)  # Label 1 indicates these sentences are consecutive\n",
    "\n",
    "            # Remove the used indices\n",
    "            available_indices.remove(index)\n",
    "            if index + 1 in available_indices:\n",
    "                available_indices.remove(index + 1)\n",
    "        else:\n",
    "            # Choose two random distinct sentences to simulate the 'not next sentence' scenario\n",
    "            indices = random.sample(available_indices, 2)\n",
    "            bert_input.append([['[CLS]']+input_sentences[indices[0]]+['[SEP]'],input_sentences[indices[1]]+ ['[SEP]']])\n",
    "            bert_label.append([['[PAD]']+input_masked_labels[indices[0]]+['[PAD]'], input_masked_labels[indices[1]]+['[PAD]']])\n",
    "            is_next.append(0)  # Label 0 indicates these sentences are not consecutive\n",
    "\n",
    "            # Remove the used indices\n",
    "            available_indices.remove(indices[0])\n",
    "            available_indices.remove(indices[1])\n",
    "\n",
    "\n",
    "\n",
    "    return bert_input, bert_label, is_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts,to_tenor=True):\n",
    "    \"\"\"\n",
    "    Prepare the final input lists for BERT training.\n",
    "    \"\"\"\n",
    "    def zero_pad_list_pair(pair_, pad='[PAD]'):\n",
    "        pair=deepcopy(pair_)\n",
    "        max_len = max(len(pair[0]), len(pair[1]))\n",
    "        #append [PAD] to each sentence in the pair till the maximum length reaches\n",
    "        pair[0].extend([pad] * (max_len - len(pair[0])))\n",
    "        pair[1].extend([pad] * (max_len - len(pair[1])))\n",
    "        return pair[0], pair[1]\n",
    "\n",
    "    #flatten the tensor\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    #transform tokens to vocab indices\n",
    "    tokens_to_index=lambda tokens: [vocab[token] for token in tokens]\n",
    "\n",
    "    bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final = [], [], [], []\n",
    "\n",
    "    for bert_input, bert_label,is_next in zip(bert_inputs, bert_labels,is_nexts):\n",
    "        # Create segment labels for each pair of sentences\n",
    "        segment_label = [[1] * len(bert_input[0]), [2] * len(bert_input[1])]\n",
    "\n",
    "        # Zero-pad the bert_input and bert_label and segment_label\n",
    "        bert_input_padded = zero_pad_list_pair(bert_input)\n",
    "        bert_label_padded = zero_pad_list_pair(bert_label)\n",
    "        segment_label_padded = zero_pad_list_pair(segment_label,pad=0)\n",
    "\n",
    "        #convert to tensors\n",
    "        if to_tenor:\n",
    "\n",
    "            # Flatten the padded inputs and labels, transform tokens to their corresponding vocab indices, and convert them to tensors\n",
    "            bert_inputs_final.append(torch.tensor(tokens_to_index(flatten(bert_input_padded)),dtype=torch.int64))\n",
    "            #bert_labels_final.append(torch.tensor(tokens_to_index(flatten(bert_label_padded)),dtype=torch.int64))\n",
    "            bert_labels_final.append(torch.tensor(tokens_to_index(flatten(bert_label_padded)),dtype=torch.int64))\n",
    "            segment_labels_final.append(torch.tensor(flatten(segment_label_padded),dtype=torch.int64))\n",
    "            is_nexts_final.append(is_next)\n",
    "\n",
    "        else:\n",
    "          # Flatten the padded inputs and labels\n",
    "            bert_inputs_final.append(flatten(bert_input_padded))\n",
    "            bert_labels_final.append(flatten(bert_label_padded))\n",
    "            segment_labels_final.append(flatten(segment_label_padded))\n",
    "            is_nexts_final.append(is_next)\n",
    "\n",
    "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
