{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project and Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up a Simple Gradio Interface to Interact with Your Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install virtualenv \n",
    "virtualenv my_env # create a virtual environment named my_env\n",
    "source my_env/bin/activate # activate my_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing necessary pacakges in my_env\n",
    "python3.11 -m pip install \\\n",
    "gradio==4.44.0 \\\n",
    "ibm-watsonx-ai==1.1.2 \\\n",
    "langchain==0.2.11 \\\n",
    "langchain-community==0.2.10 \\\n",
    "langchain-ibm==0.1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def add_numbers(Num1, Num2):\n",
    "    return Num1 + Num2\n",
    "\n",
    "# Define the interface\n",
    "demo = gr.Interface(\n",
    "    fn=add_numbers, \n",
    "    inputs=[gr.Number(), gr.Number()], # Create two numerical input fields where users can enter numbers\n",
    "    outputs=gr.Number() # Create numerical output fields\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch(server_name=\"127.0.0.1\", server_port= 7860)\n",
    "\n",
    "# To run use command python3.11 gradio_demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "# Model and project settings\n",
    "model_id = 'mistralai/mixtral-8x7b-instruct-v01' # Directly specifying the model\n",
    "\n",
    "# Set necessary parameters\n",
    "parameters = {\n",
    "    GenParams.MAX_NEW_TOKENS: 256,  # Specifying the max tokens you want to generate\n",
    "    GenParams.TEMPERATURE: 0.5, # This randomness or creativity of the model's responses\n",
    "}\n",
    "\n",
    "project_id = \"skills-network\"\n",
    "\n",
    "# Wrap up the model into WatsonxLLM inference\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=project_id,\n",
    "    params=parameters,\n",
    ")\n",
    "\n",
    "# Get the query from the user input\n",
    "query = input(\"Please enter your query: \")\n",
    "\n",
    "# Print the generated response\n",
    "print(watsonx_llm.invoke(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from langchain_ibm import WatsonxLLM\n",
    "import gradio as gr\n",
    "\n",
    "# Model and project settings\n",
    "model_id = 'mistralai/mixtral-8x7b-instruct-v01' # Directly specifying the model\n",
    "\n",
    "# Set necessary parameters\n",
    "parameters = {\n",
    "    GenParams.MAX_NEW_TOKENS: 256,  # Specifying the max tokens you want to generate\n",
    "    GenParams.TEMPERATURE: 0.5, # This randomness or creativity of the model's responses\n",
    "}\n",
    "\n",
    "project_id = \"skills-network\"\n",
    "\n",
    "# Wrap up the model into WatsonxLLM inference\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=project_id,\n",
    "    params=parameters,\n",
    ")\n",
    "\n",
    "# Function to generate a response from the model\n",
    "def generate_response(prompt_txt):\n",
    "    generated_response = watsonx_llm.invoke(prompt_txt)\n",
    "    return generated_response\n",
    "\n",
    "# Create Gradio interface\n",
    "chat_application = gr.Interface(\n",
    "    fn=generate_response,\n",
    "    allow_flagging=\"never\",\n",
    "    inputs=gr.Textbox(label=\"Input\", lines=2, placeholder=\"Type your question here...\"),\n",
    "    outputs=gr.Textbox(label=\"Output\"),\n",
    "    title=\"Watsonx.ai Chatbot\",\n",
    "    description=\"Ask any question and the chatbot will try to answer.\"\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "chat_application.launch(server_name=\"127.0.0.1\", server_port= 7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
